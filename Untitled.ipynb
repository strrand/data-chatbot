{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59995db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/strrand/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e977a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def chat(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\", \n",
    "        prompt=prompt, \n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].text.strip()\n",
    "    #message = response_text.split(\"\\n\\n\")[0]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83e46505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to train a model to predict the next character in a sequence based on the current state of the sequence. For example, the sequence: “a,b,c,d,e,f,g,h” should predict “h” as the next character.\\n\\nI have tried to find a simple example in tensorflow, with no success. I already have an implementation in tensorflow, but I want to use tensorflow as a backend and not as a library.\\n\\nI thought you might want to add a simple example to tensorflow-gpu, but it seems that you can’t simply add a new example.\\n\\nPlease help!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Can you please write me a simple LSTM deep learning model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae41e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7840004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6xKEA4KMAEtG7CgBk88uxUanNH8Rb at 0x7fea7361b1f0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas, USA.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679597126,\n",
       "  \"id\": \"chatcmpl-6xKEA4KMAEtG7CgBk88uxUanNH8Rb\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 20,\n",
       "    \"prompt_tokens\": 56,\n",
       "    \"total_tokens\": 76\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8da47fc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmessages\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "messages['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6897ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def chat(input_text):\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": \"You are a helpful assistant working at LeasePlan Sweden AB and have good knowledge about the leasing business.\"},\n",
    "        {\"role\": \"User\", \"content\": input_text},\n",
    "    ]\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=\"Conversation:\\n\" + '\\n'.join([f'{m[\"content\"]}' for m in messages]) + \"\\n\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].text.strip()\n",
    "    messages.append({\"role\": \"Assistant\", \"content\": message})\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98037165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you lease a Tesla Model 3 for 3 years, it would cost you approximately $369 per month.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What would a Tesla Model 3 cost me if I lease for 3 years?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "419307df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm a chatbot. What can I help you with today?\n",
      "You: Hi i want to know what type of brands i can lease from you at leaseplan?\n",
      "Chatbot: There are many brands that LeasePlan Sweden AB offers leasing for. Some of these brands include Volvo, BMW, Audi, and Mercedes-Benz.\n",
      "You: How much would a volvo v90 XC cost me?\n",
      "Chatbot: The Volvo V90 XC would cost you around $379 per month.\n",
      "You: please answer me in sek\n",
      "Chatbot: How much does it cost to lease a car in Sweden?\n",
      "\n",
      "A lease typically costs between 1,500-2,500 SEK per month.\n",
      "You: THanks\n",
      "Chatbot: You're welcome!\n",
      "You: Can I get the phone number to leaseplan?\n",
      "Chatbot: The phone number to LeasePlan Sweden AB is +46 (0)8-638 70 00.\n",
      "You: what department could anwser my question regarding fuel cards?\n",
      "Chatbot: The customer service department would be the best department to answer your question regarding fuel cards.\n",
      "You: okay thanks thats it\n",
      "Chatbot: Thank you for your question. As far as I know, LeasePlan Sweden AB is a company that specializes in leasing vehicles. I'm not entirely sure what else you would like to know about them, but hopefully this is helpful.\n",
      "You: its enough\n",
      "Chatbot: A:\n",
      "\n",
      "Thank you for your question. From what you have said, it sounds like you have good knowledge about the leasing business and would be able to help out at LeasePlan Sweden AB. However, if you are not certain about something, it is always best to ask a supervisor or another member of staff for help.\n",
      "You: bye\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# define the chat function\n",
    "def chatbot():\n",
    "    # greet the user\n",
    "    print(\"Hi, I'm a chatbot. What can I help you with today?\")\n",
    "\n",
    "    # start the conversation loop\n",
    "    while True:\n",
    "        # get the user's input\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        # exit the loop if the user says \"bye\"\n",
    "        if user_input.lower() == \"bye\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # generate a response using the chat function\n",
    "        response = chat(user_input)\n",
    "\n",
    "        # print the response\n",
    "        print(\"Chatbot: \" + response)\n",
    "\n",
    "        # add a delay to simulate natural conversation\n",
    "        time.sleep(1)\n",
    "\n",
    "# run the chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30aae6ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Additional properties are not allowed ('examples', 'prompt' were unexpected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mFine-tune the GPT-3 language model on a dataset of leasing-related text.\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mInput: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe lease for my car is expiring soon, what are my options?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have several options for extending or terminating your lease. Here are some of the most common ones: ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fine-tune the model using the OpenAI API\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFineTune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_response\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use the fine-tuned model for chatbot responses\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(input_text):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_resources/abstract/createable_api_resource.py:57\u001b[0m, in \u001b[0;36mCreateableAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m     48\u001b[0m ):\n\u001b[1;32m     49\u001b[0m     requestor, url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_create_requestor(\n\u001b[1;32m     50\u001b[0m         api_key,\n\u001b[1;32m     51\u001b[0m         api_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m         organization,\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 57\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m     62\u001b[0m         response,\n\u001b[1;32m     63\u001b[0m         api_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         plain_old_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mplain_old_data,\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Additional properties are not allowed ('examples', 'prompt' were unexpected)"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Load your training data into a list of strings\n",
    "training_data = [\"Formal extension of your contract with 6 months.\", \"Return your car and order a new one.\", \"Change your product to a Used Car Lease or Flexiplan.\"]\n",
    "\n",
    "# Define your fine-tuning prompt\n",
    "prompt = \"\"\"\n",
    "Fine-tune the GPT-3 language model on a dataset of leasing-related text.\n",
    "Input: \"The lease for my car is expiring soon, what are my options?\"\n",
    "Output: \"You have several options for extending or terminating your lease. Here are some of the most common ones: ...\"\n",
    "\"\"\"\n",
    "\n",
    "# Fine-tune the model using the OpenAI API\n",
    "model = openai.FineTune.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=prompt,\n",
    "    examples=[\n",
    "        {\"input\": example_text, \"output\": example_response}\n",
    "        for example_text, example_response in zip(training_data, training_data)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the fine-tuned model for chatbot responses\n",
    "def chat(input_text):\n",
    "    messages = [{\"role\": \"User\", \"content\": input_text}]\n",
    "    response = openai.Completion.create(\n",
    "        engine=model.id,\n",
    "        prompt=\"\\n\".join([f'{m[\"role\"]}: {m[\"content\"]}' for m in messages]),\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    message = response.choices[0].text.strip()\n",
    "    messages.append({\"role\": \"Assistant\", \"content\": message})\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0b781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
